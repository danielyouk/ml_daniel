---
title: "Machine Learning with Python: Evaluation"
format: 
    html:
        fontsize: 13pt
        fig.process: true
        number-sections: true
        css: custom.css
        geometry: margin=1in
        # df-print: paged
# fig-caption-style: 
# text-align: center;
editor_options:
  chunk_output_type: console
---

# Part2

```{r, include=FALSE}
library(reticulate)
reticulate::use_condaenv(condaenv = "/opt/miniconda3/envs/ml-mac")
```

## Evaluation Metric
### Accuracy

```{python}
def accuracy(y_true, y_pred): 
    """ 
    Function to calculate accuracy 
    :param y_true: list of true values 
    :param y_pred: list of predicted values 
    :return: accuracy score 
    """ 
    # initialize a simple counter for correct predictions 
    correct_counter = 0 
    # loop over all elements of y_true 
    # and y_pred "together" 
    for yt, yp in zip(y_true, y_pred): 
        if yt == yp: 
            # if prediction is equal to truth, increase the counter 
            correct_counter += 1 
 
    # return accuracy 
    # which is correct predictions over the number of samples 
    return correct_counter / len(y_true) 
```

- If data is skewed, it's not a good idea to use accuracy. For instance, for the case of fraud detection, if saying always not fraud, then the accuracy can be more than 99%.

                    Accuracy Score = (TP + TN) / (TP + TN + FP + FN) 

```{python}
def true_positive(y_true, y_pred): 
    """ 
    Function to calculate True Positives 
    :param y_true: list of true values 
    :param y_pred: list of predicted values 
    :return: number of true positives 
    """ 
    # initialize 
    tp = 0 
    for yt, yp in zip(y_true, y_pred): 
        if yt == 1 and yp == 1: 
            tp += 1 
    return tp 
 
def true_negative(y_true, y_pred): 
    """ 
    Function to calculate True Negatives 
    :param y_true: list of true values 
    :param y_pred: list of predicted values 
    :return: number of true negatives 
    """ 
    # initialize 
    tn = 0 
    for yt, yp in zip(y_true, y_pred): 
        if yt == 0 and yp == 0: 
            tn += 1 
    return tn 
 
def false_positive(y_true, y_pred): 
    """ 
    Function to calculate False Positives 
    :param y_true: list of true values 
    :param y_pred: list of predicted values 
    :return: number of false positives 
    """ 
    # initialize 
    fp = 0 
    for yt, yp in zip(y_true, y_pred): 
        if yt == 0 and yp == 1: 
            fp += 1 
    return fp 
 
def false_negative(y_true, y_pred): 
    """ 
    Function to calculate False Negatives 
    :param y_true: list of true values 
    :param y_pred: list of predicted values 
    :return: number of false negatives 
    """ 
    # initialize 
    fn = 0 
    for yt, yp in zip(y_true, y_pred): 
        if yt == 1 and yp == 0: 
            fn += 1 
    return fn

```
```{python}
def accuracy_v2(y_true, y_pred): 
    """ 
    Function to calculate accuracy using tp/tn/fp/fn 
    :param y_true: list of true values 
    :param y_pred: list of predicted values 
    :return: accuracy score 
    """ 
    tp = true_positive(y_true, y_pred) 
    fp = false_positive(y_true, y_pred) 
    fn = false_negative(y_true, y_pred) 
    tn = true_negative(y_true, y_pred) 
    accuracy_score = (tp + tn) / (tp + tn + fp + fn) 
    return accuracy_score
```
### Precision
                    Precision = TP / (TP + FP) 

```{python}
def precision(y_true, y_pred): 
    """ 
    Function to calculate precision 
    :param y_true: list of true values 
    :param y_pred: list of predicted values 
    :return: precision score 
    """ 
    tp = true_positive(y_true, y_pred) 
    fp = false_positive(y_true, y_pred) 
    precision = tp / (tp + fp) 
    return precision 
```

### Recall (TPR: True Positive Rate or Sensitivity)
                    Recall = TP / (TP + FN) 
```{python}
def recall(y_true, y_pred): 
    """ 
    Function to calculate recall 
    :param y_true: list of true values 
    :param y_pred: list of predicted values 
    :return: recall score 
    """ 
    tp = true_positive(y_true, y_pred) 
    fn = false_negative(y_true, y_pred) 
    recall = tp / (tp + fn) 
    return recall
```

```{python}
y_true = [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0]
y_pred = [0.02638412, 0.11114267, 0.31620708, 0.0490937,  0.0191491,  0.17554844, 0.15952202, 0.03819563, 0.11639273,0.079377, 0.08584789, 0.39095342, 0.27259048, 0.03447096, 0.04644807, 0.03543574, 0.18521942, 0.05934905, 0.61977213, 0.33056815]
thresholds = [0.0490937 , 0.05934905, 0.079377, 0.08584789, 0.11114267, 0.11639273, 0.15952202, 0.17554844, 0.18521942, 0.27259048, 0.31620708, 0.33056815, 0.39095342, 0.61977213] 
```



```{python}
precisions = [] 
recalls = [] 
# for every threshold, calculate predictions in binary 
# and append calculated precisions and recalls 
# to their respective lists 
for i in thresholds: 
    temp_prediction = [1 if x >= i else 0 for x in y_pred] 
    p = precision(y_true, temp_prediction) 
    r = recall(y_true, temp_prediction) 
    precisions.append(p) 
    recalls.append(r)
```


```{python}
import matplotlib.pyplot as plt
# create precision-recall curve by points
plt.figure()
plt.plot(recalls, precisions, marker='.')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall curve')
plt.show()
```

### f1 score
                    F1 = 2PR / (P + R) 


```{python}
def f1(y_true, y_pred): 
    """ 
    Function to calculate f1 score 
    :param y_true: list of true values 
    :param y_pred: list of predicted values 
    :return: f1 score 
    """ 
    p = precision(y_true, y_pred) 
    r = recall(y_true, y_pred) 
 
    score = 2 * p * r / (p + r) 
 
    return score
```


```{python}
f1_scores = []
for i in thresholds: 
    temp_prediction = [1 if y >= i else 0 for y in y_pred] 
    f1_score = f1(y_true, temp_prediction) 
    f1_scores.append(f1_score) 
```


```{python}
plt.figure()
plt.plot(thresholds, f1_scores, marker='.')
plt.xlabel('threshold')
plt.ylabel('f1_score')
plt.title('threshole-f1_score curve')
plt.show()
```

* When dealing with datasets that have skewed targets, we should look at F1 (or precision and recall) instead of accuracy.

### false positive rate

```{python}
def fpr(y_true, y_pred): 
    """ 
    Function to calculate fpr 
    :param y_true: list of true values 
    :param y_pred: list of predicted values 
    :return: fpr 
    """ 
    fp = false_positive(y_true, y_pred) 
    tn = true_negative(y_true, y_pred) 
    return fp / (tn + fp)
```